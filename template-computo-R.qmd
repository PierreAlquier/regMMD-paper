# Introduction

In some models, popular estimators such as the maximum likelihood estimator (MLE) can become very unstable in the presence of outliers. This has motivated research into robust estimation procedures that would not suffer from this issue. Various notions of robustness have been proposed, depending on the type of contamination in the data. Notably, the Huber contamination model [@huber1964robust] considers random outliers while, more recently, stricter notions have been proposed to ensure robustness against adversarial contamination of the data.

The maximum mean discrepancy (MMD) is a kernel-based metric that has received considerable attention in the past 15 years. It allows for the development of tools for nonparametric tests and estimation [@pmlr-v48-chwialkowski16;@gretton2007kernel]. We refer the reader to [@muandet2017kernel] for a comprehensive introduction to MMD and its applications. A recent series of papers has suggested that minimum distance estimators (MDEs) based on the MMD are robust to both Huber and adversarial contamination. These estimators were initially proposed for training generative AI [@dziugaite2015training;@sutherland2016generative;@li2017mmd], and the study of their statistical properties for parametric estimation, including robustness, was initiated by [@briol2019statistical;@cherief2019finite;@alquier2024universal]. We also point out that the MMD has been successfully used to define robust estimators that are not MDEs, such as bootstrap methods based on MMD [@dellaporta2022robust] or Approximate Bayesian Computation [@legramanti2022concentration].

Unfortunately, we are not aware of any software that allows for the computation of MMD-based MDEs. To make these tools available to the statistical community, we developed the `R` package called `regMMD`. This package allows for the minimization of the MMD distance between the empirical distribution and a statistical model. Various parametric models can be fitted, including continuous distributions such as Gaussian and gamma, and discrete distributions such as Poisson and binomial. Many regression models are also available, including linear, logistic, and gamma regression. The `regMMD` package is available on the `CRAN` website [@R-base]: [regMMD page](https://cran.r-project.org/web/packages/regMMD/)

The optimization is based on the strategies proposed by [@briol2019statistical;@cherief2019finite;@alquier2024universal]. For some models  we have an explicit formula for the gradient of the MMD, in which   case  we   use gradient descent, see e.g. [@boyd2004convex;@nesterov2018lectures] to perform the optimization. For most models  such a formula does not exist, but we can however   approximate the gradient without bias by Monte Carlo sampling. This allows to use the stochastic gradient algorithm of [@robbins1951stochastic], that is one of the most popular estimation methods in machine learning [@bottou-mlss-2004]. We refer to the reader to  [@wright2018optimization;@duchi2018introductory] and Chapter 5 in [@bach2024learning] for comprehensive introductions to otpimization for statistics and machine learning, including stochastic optimization methods.

The paper is organized as follows. In @sec-stat we briefly recall the construction of the MMD metric and the MDE estimators based on the MMD. In @sec-content  we detail the content of the package `regMMD`: the available models and kernels, and the optimization procedures used in each case. Finally, in @sec-examples we provide examples of applications of `regMMD`. Note that these experiments are not meant to be a comprehensive comparison of MMD to other robust estimation procedures. Exhaustive comparisons can be found in [@briol2019statistical;@cherief2019finite;@alquier2024universal]. The objective is simply to illustrate the use of `regMMD` through pedagogical examples.

# Statistical background {#sec-stat}

## Parametric estimation {#sec-para}

Let $X_1,\dots,X_n$ be $\mathcal{X}$-valued random variables identically distributed according to some probability distribution $P^0$, and let $(P_\theta,\theta\in\Theta)$ be a statistical model. Given a metric $d$ on probability distributions, we are looking for an estimator of $\theta_0 \in\argmin_{\theta\in\Theta} d(P_\theta,P^0)$ when such a minimum exists. Letting   $\hat{P}_n=\frac{1}{n}\sum_{i=1}^n \delta_{X_i}$ denote the empirical probability distribution, the  minimum distance estimator (MDE) $\hat{\theta}$ is defined as follows:
$$
\hat{\theta} \in \argmin_{\theta\in\Theta} d(P_\theta,\hat{P}_n ).
$$
The robustness properties of MDEs for well chosen distances was studied as early as in [@wolfowitz1957minimum;@parr1980minimum;@yatracos2022limitations]. When $d(P_\theta,\hat{P}_n )$ has no minimum, the definition can be replaced by an $\varepsilon$-approximate minimizer, without consequences on the the properties of the estimator, as shown in to [@briol2019statistical;@cherief2019finite].

Let $\mathcal{H}$ be a Hilbert space, let $\|\cdot\|_{\mathcal{H}}$ and $\left<\cdot,\cdot\right>_{\mathcal{H}}$  denote the associated norms and scalar products, respectively, and let $\varphi:\mathcal{X}\rightarrow\mathcal{H}$. Then, for any probability distribution $P$ on $\mathcal{X}$ such that $\mathbb{E}_{X\sim P}[\|\varphi(X)\|_{\mathcal{H}}]<+\infty$ we can define the mean embedding $\mu(P)=\mathbb{E}_{X\sim P}[\varphi(X)]$. When the   mean embedding $\mu(P)$ is defined for any probability distribution $P$ (e.g. because the map $\varphi$ is bounded in $\mathcal{H}$), for any probability distributions $P$ and $Q$ we put
$$
\mathbb{D}(P,Q) := \left\| \mu(P) - \mu(Q) \right\|_{\mathcal{H}}.
$$
Letting $k(x,y)=\left<\varphi(x),\varphi(y)\right>_{\mathcal{H}}$, it appears that $\mathbb{D}(P,Q)$ depends on $\varphi$ only through $k$, as we can rewrite:
$$
\mathbb{D}^2(P,Q)
= \mathbb{E}_{X,X'\sim P} [k(X,X')] -2 \mathbb{E}_{X\sim P,X'\sim Q} [k(X,X')] + \mathbb{E}_{X,X'\sim Q} [k(X,X')].
$$ {#eq-MMD}
When $\mathcal{H}$ is actually a RKHS for the kernel $k$ (see [@muandet2017kernel] for a definition), $\mathbb{D}(P,Q)$ is called the maximum mean discrepancy (MMD) between $P$ and $Q$. A condition on $k$ (universal kernel) ensures that the map $\mu$ is injective, and thus that $\mathbb{D}$ satisfies the axioms of a metric. Examples of universal kernels are known, such as the Gaussian kernel $k(x,y)=\exp(-\|x-y\|^2/\gamma^2)$ or the Laplace kernel $k(x,y)=\exp(-\|x-y\|/\gamma)$, see [@muandet2017kernel] for more examples and references to the proofs.

The properties of MDEs based on MMD were studied in [@briol2019statistical;@cherief2019finite]. In particular, when the kernel $k$ is bounded, this estimator enjoys very strong robustness properties. We cite the following very simple result.

::: {#thm-cherief}

## special case of Theorem 3.1 in [@cherief2019finite]

Assume $X_1,\dots,X_n$ are i.i.d. from $P^0$. Assume the kernel $k$ is bounded by $1$. Then
$$
\mathbb{E} \left[ \mathbb{D}\left( P_{\hat{\theta}}, P^0 \right) \right]
\leq \inf_{\theta\in\Theta} \mathbb{D}\left( P_{\theta}, P^0 \right) + \frac{2}{\sqrt{n}}.
$$
:::

Additional non-asymptotic results can be found in [@briol2019statistical;@cherief2019finite]. In particular, Theorem 3.1 of [@cherief2019finite] also covers non independent observations (time series). An asymptotic study of $\hat{\theta}$, including conditions for asymptotic normality, can be found in [@briol2019statistical]. All these works provide strong theoretical evidence that $\hat{\theta}$ is very robust to random and adversarial contamination of the data, and this is supported by empirical evidence.

## Regression


Let us now consider a regression setting: we observe $(X_1,Y_1),\dots,(X_n,Y_n)$ in $\mathcal{X}\times \mathcal{Y}$ and we want to estimate the conditional distribution $P^0_{Y|X=x}$ of $Y$ given $X=x$ for any $x$. %A direct application of the method in the previous section to the random variables $(X_1,Y_1),\dots, (X_n,Y_n)$ would   lead to the estimation of the joint distribution of the pair $(X,Y)$, which is not the objective of regression models.
To this end we consider a statistical model $(P_\beta,\beta\in\mathcal{B})$ and model the conditional distribution of $Y$ given $X=x$ by $(P_{\beta(x,\theta)},\theta\in\Theta)$ where $\beta(\cdot,\cdot)$ is a specified function $\mathcal{X}\times\Theta \rightarrow \mathcal{B}$. The first estimator proposed by [@alquier2024universal] is:
$$
\hat{\theta}_{\mathrm{reg}}\in\argmin_{\theta\in\Theta} \mathbb{D}\left(
\frac{1}{n}\sum_{i=1}^n \delta_{X_i} \otimes P_{\beta(X_i,\theta)},
\frac{1}{n}\sum_{i=1}^n \delta_{X_i} \otimes \delta_{Y_i}
\right)
$$
where $\mathbb{D}$ is the MMD defined by a product kernel, that is  a kernel of the form $k((x,y),(x',y'))=k_X(x,x')k_Y(y,y')$ (non-product kernels are theoretically possible, but not implemented in the package). Asymptotic and non-asymptotic properties of $\hat{\theta}$ are studied in [@alquier2024universal]. The computation of $\hat{\theta}$ is however slow when the sample size $n$ is large, as it can be shown that  the criterion defining this estimator is the sum of $n^2$ terms.

By contrast, the following alternative estimator 
$$
\tilde{\theta}_{\mathrm{reg}}\in\argmin_{\theta\in\Theta} \frac{1}{n}\sum_{i=1}^n \mathbb{D}\left(
\delta_{X_i} \otimes P_{\beta(X_i,\theta)},
\delta_{X_i} \otimes \delta_{Y_i}
\right),
$$
has the advantage to be defined through a criterion which is a sum of only $n$ terms. Intuitively, this estimator can be interpreted as a special case of $\hat{\theta}_{\mathrm{reg}}$ where $k_X(x,x')=\mathbf{1}_{ \{x=x'\}}$. An asymptotic study of $\tilde{\theta}_{\mathrm{reg}}$ is provided by [@alquier2024universal]. The theory and the experiments suggest that both estimators are robust, but that $\hat{\theta}_{\mathrm{reg}}$ is more robust than $\tilde{\theta}_{\mathrm{reg}}$. However, for computations reasons, for large sample sizes $n$ ($n>5\,000$, say) only the latter estimator  can be computed in a reasonable amount of time.

# Package content and implementation {#sec-content}

The package `regMMD` allows to compute the above estimators  in a large number of classical models. We first provide an overview of the two main functions with their default settings, and then give some details on their implementations and parameters. These functions have many options related to the choice of kernels, the choice of the bandwidth parameters and of the parameters of the optimization algorithms used to compute the estimators. To save space, in this section we only discuss the options that are fundamental from a statistical perspective. We refer the reader to the package documentation for a full description of the available options.

We start by loading the package and fixing the seed to ensure reproducibility.
```{r r-code0, echo=TRUE}
require("regMMD")
set.seed(1)
```

## Overview of the function `mmd_est`

The function `mmd_est` performs parametric estimation as described in @sec-para. Its required arguments are  the data `x` and the type of model `model` (see @sec-models for the list of available models). Each model implemented in the package has one or two  parameters, namely `par1` and `par2`. If the   model contains a parameter that is fixed  (i.e. not estimated from the data) then its value must be specified by the user. On the other hand, a value for a parameter   that we want to estimate from the data  does not have to be given as an input. If, however, a value is provided then it is used to initialize  the optimization algorithm that serves at computing the estimator (see below). Otherwise  an initialization by default is used.

For example, there are three Gaussian univariate models: `Gaussian.loc`, `Gaussian.scale` and `Gaussian`. In each model `par1` is the mean and `par2` is the standard deviation. We will use the following data:
```{r r-code1, echo=TRUE}
x = rnorm(100,1.9,1)
hist(x)
```

In the `Gaussian` model the two parameters `par1` and `par2` are estimated from the data and the MMD estimator of $\theta=$(`par1`,`par1`) can be computed as follows:

```{r r-code2, echo=TRUE}
estim = mmd_est(x,model="Gaussian")
summary(estim)
```

By contrast, if we enter
```{r r-code3, echo=TRUE}
estim = mmd_est(x,model="Gaussian",par1=0,par2=1)
summary(estim)
```
we simply enforce the optimization algorithm that serves at computing the estimator to use $\theta_0=(0,1)$ as starting value. In the `Gaussian.loc` model only the location parameter (the mean) is estimated. Thus, to compute $\theta=$ `par1` it is necessary to specify the standard deviation `par2`. For instance,
```{r r-code4, echo=TRUE}
estim = mmd_est(x,model="Gaussian.loc",par2=1)
summary(estim)
```
will estimate $\theta$ in the model $\mathcal{N}(\theta,1)$. If we provide a value for `par1` then the optimization algorithm used to compute $\hat{\theta}$ will use $\theta_0=$`par1` as starting value. Finally, In the `Gaussian.scale` model only the scale parameter (standard deviation) is estimated. That is, to estimate  $\theta=$`par2` in e.g. the $\mathcal{N}(4,\theta^2)$ distribution we can use
```{r r-code5, echo=TRUE}
estim = mmd_est(x,model="Gaussian.scale",par1=2)
summary(estim)
```

## Overview of the function `mmd_reg`

## Kernels and bandwidth parameters

## Optimization methods

## Available models {#sec-models}

# Detailed examples {#sec-examples}

## Toy example: robust estimation in the univariate Gaussian model

## Robust linear regression


# References {.unnumbered}

::: {#refs}
:::

# Session information {.appendix .unnumbered}

```{r session-info}
sessionInfo()
```

