# Introduction

In some models, popular estimators such as the maximum likelihood estimator (MLE) can become very unstable in the presence of outliers. This has motivated research into robust estimation procedures that would not suffer from this issue. Various notions of robustness have been proposed, depending on the type of contamination in the data. Notably, the Huber contamination model [@huber1964robust] considers random outliers while, more recently, stricter notions have been proposed to ensure robustness against adversarial contamination of the data.

The maximum mean discrepancy (MMD) is a kernel-based metric that has received considerable attention in the past 15 years. It allows for the development of tools for nonparametric tests and estimation [@pmlr-v48-chwialkowski16;@gretton2007kernel]. We refer the reader to [@muandet2017kernel] for a comprehensive introduction to MMD and its applications. A recent series of papers has suggested that minimum distance estimators (MDEs) based on the MMD are robust to both Huber and adversarial contamination. These estimators were initially proposed for training generative AI [@dziugaite2015training;@sutherland2016generative;@li2017mmd], and the study of their statistical properties for parametric estimation, including robustness, was initiated by [@briol2019statistical;@cherief2019finite;@alquier2024universal]. We also point out that the MMD has been successfully used to define robust estimators that are not MDEs, such as bootstrap methods based on MMD [@dellaporta2022robust] or Approximate Bayesian Computation [@legramanti2022concentration].

Unfortunately, we are not aware of any software that allows for the computation of MMD-based MDEs. To make these tools available to the statistical community, we developed the `R` package called `regMMD`. This package allows for the minimization of the MMD distance between the empirical distribution and a statistical model. Various parametric models can be fitted, including continuous distributions such as Gaussian and gamma, and discrete distributions such as Poisson and binomial. Many regression models are also available, including linear, logistic, and gamma regression. The `regMMD` package is available on the `CRAN` website [@R-base]: [regMMD page](https://cran.r-project.org/web/packages/regMMD/)

The optimization is based on the strategies proposed by [@briol2019statistical;@cherief2019finite;@alquier2024universal]. For some models  we have an explicit formula for the gradient of the MMD, in which   case  we   use gradient descent, see e.g. [@boyd2004convex;@nesterov2018lectures] to perform the optimization. For most models  such a formula does not exist, but we can however   approximate the gradient without bias by Monte Carlo sampling. This allows to use the stochastic gradient algorithm of [@robbins1951stochastic], that is one of the most popular estimation methods in machine learning [@bottou-mlss-2004]. We refer to the reader to  [@wright2018optimization;@duchi2018introductory] and Chapter 5 in [@bach2024learning] for comprehensive introductions to otpimization for statistics and machine learning, including stochastic optimization methods.

The paper is organized as follows. In @sec-stat we briefly recall the construction of the MMD metric and the MDE estimators based on the MMD. In @sec-content  we detail the content of the package `regMMD`: the available models and kernels, and the optimization procedures used in each case. Finally, in @sec-examples we provide examples of applications of `regMMD`. Note that these experiments are not meant to be a comprehensive comparison of MMD to other robust estimation procedures. Exhaustive comparisons can be found in [@briol2019statistical;@cherief2019finite;@alquier2024universal]. The objective is simply to illustrate the use of `regMMD` through pedagogical examples.

# Statistical background {#sec-stat}

## Parametric estimation {#sec-para}

Let $X_1,\dots,X_n$ be $\mathcal{X}$-valued random variables identically distributed according to some probability distribution $P^0$, and let $(P_\theta,\theta\in\Theta)$ be a statistical model. Given a metric $d$ on probability distributions, we are looking for an estimator of $\theta_0 \in\arg\min_{\theta\in\Theta} d(P_\theta,P^0)$ when such a minimum exists. Letting   $\hat{P}_n=\frac{1}{n}\sum_{i=1}^n \delta_{X_i}$ denote the empirical probability distribution, the  minimum distance estimator (MDE) $\hat{\theta}$ is defined as follows:
$$
\hat{\theta} \in \arg\min_{\theta\in\Theta} d(P_\theta,\hat{P}_n ).
$$
The robustness properties of MDEs for well chosen distances was studied as early as in [@wolfowitz1957minimum;@parr1980minimum;@yatracos2022limitations]. When $d(P_\theta,\hat{P}_n )$ has no minimum, the definition can be replaced by an $\varepsilon$-approximate minimizer, without consequences on the the properties of the estimator, as shown in to [@briol2019statistical;@cherief2019finite].

Let $\mathcal{H}$ be a Hilbert space, let $\|\cdot\|_{\mathcal{H}}$ and $\left<\cdot,\cdot\right>_{\mathcal{H}}$  denote the associated norms and scalar products, respectively, and let $\varphi:\mathcal{X}\rightarrow\mathcal{H}$. Then, for any probability distribution $P$ on $\mathcal{X}$ such that $\mathbb{E}_{X\sim P}[\|\varphi(X)\|_{\mathcal{H}}]<+\infty$ we can define the mean embedding $\mu(P)=\mathbb{E}_{X\sim P}[\varphi(X)]$. When the   mean embedding $\mu(P)$ is defined for any probability distribution $P$ (e.g. because the map $\varphi$ is bounded in $\mathcal{H}$), for any probability distributions $P$ and $Q$ we put
$$
\mathbb{D}(P,Q) := \left\| \mu(P) - \mu(Q) \right\|_{\mathcal{H}}.
$$
Letting $k(x,y)=\left<\varphi(x),\varphi(y)\right>_{\mathcal{H}}$, it appears that $\mathbb{D}(P,Q)$ depends on $\varphi$ only through $k$, as we can rewrite:
$$
\mathbb{D}^2(P,Q)
= \mathbb{E}_{X,X'\sim P} [k(X,X')] -2 \mathbb{E}_{X\sim P,X'\sim Q} [k(X,X')] + \mathbb{E}_{X,X'\sim Q} [k(X,X')].
$$ {#eq-MMD}
When $\mathcal{H}$ is actually a RKHS for the kernel $k$ (see [@muandet2017kernel] for a definition), $\mathbb{D}(P,Q)$ is called the maximum mean discrepancy (MMD) between $P$ and $Q$. A condition on $k$ (universal kernel) ensures that the map $\mu$ is injective, and thus that $\mathbb{D}$ satisfies the axioms of a metric. Examples of universal kernels are known, such as the Gaussian kernel $k(x,y)=\exp(-\|x-y\|^2/\gamma^2)$ or the Laplace kernel $k(x,y)=\exp(-\|x-y\|/\gamma)$, see [@muandet2017kernel] for more examples and references to the proofs.

The properties of MDEs based on MMD were studied in [@briol2019statistical;@cherief2019finite]. In particular, when the kernel $k$ is bounded, this estimator enjoys very strong robustness properties. We cite the following very simple result.

::: {#thm-cherief}

## special case of Theorem 3.1 in [@cherief2019finite]

Assume $X_1,\dots,X_n$ are i.i.d. from $P^0$. Assume the kernel $k$ is bounded by $1$. Then
$$
\mathbb{E} \left[ \mathbb{D}\left( P_{\hat{\theta}}, P^0 \right) \right]
\leq \inf_{\theta\in\Theta} \mathbb{D}\left( P_{\theta}, P^0 \right) + \frac{2}{\sqrt{n}}.
$$
:::

Additional non-asymptotic results can be found in [@briol2019statistical;@cherief2019finite]. In particular, Theorem 3.1 of [@cherief2019finite] also covers non independent observations (time series). An asymptotic study of $\hat{\theta}$, including conditions for asymptotic normality, can be found in [@briol2019statistical]. All these works provide strong theoretical evidence that $\hat{\theta}$ is very robust to random and adversarial contamination of the data, and this is supported by empirical evidence.

## Regression {#sec-reg}

Let us now consider a regression setting: we observe $(X_1,Y_1),\dots,(X_n,Y_n)$ in $\mathcal{X}\times \mathcal{Y}$ and we want to estimate the conditional distribution $P^0_{Y|X=x}$ of $Y$ given $X=x$ for any $x$. %A direct application of the method in the previous section to the random variables $(X_1,Y_1),\dots, (X_n,Y_n)$ would   lead to the estimation of the joint distribution of the pair $(X,Y)$, which is not the objective of regression models.
To this end we consider a statistical model $(P_\beta,\beta\in\mathcal{B})$ and model the conditional distribution of $Y$ given $X=x$ by $(P_{\beta(x,\theta)},\theta\in\Theta)$ where $\beta(\cdot,\cdot)$ is a specified function $\mathcal{X}\times\Theta \rightarrow \mathcal{B}$. The first estimator proposed by [@alquier2024universal] is:
$$
\hat{\theta}_{\mathrm{reg}}\in\arg\min_{\theta\in\Theta} \mathbb{D}\left(
\frac{1}{n}\sum_{i=1}^n \delta_{X_i} \otimes P_{\beta(X_i,\theta)},
\frac{1}{n}\sum_{i=1}^n \delta_{X_i} \otimes \delta_{Y_i}
\right)
$$
where $\mathbb{D}$ is the MMD defined by a product kernel, that is  a kernel of the form $k((x,y),(x',y'))=k_X(x,x')k_Y(y,y')$ (non-product kernels are theoretically possible, but not implemented in the package). Asymptotic and non-asymptotic properties of $\hat{\theta}$ are studied in [@alquier2024universal]. The computation of $\hat{\theta}$ is however slow when the sample size $n$ is large, as it can be shown that  the criterion defining this estimator is the sum of $n^2$ terms.

By contrast, the following alternative estimator 
$$
\tilde{\theta}_{\mathrm{reg}}\in\arg\min_{\theta\in\Theta} \frac{1}{n}\sum_{i=1}^n \mathbb{D}\left(
\delta_{X_i} \otimes P_{\beta(X_i,\theta)},
\delta_{X_i} \otimes \delta_{Y_i}
\right),
$$
has the advantage to be defined through a criterion which is a sum of only $n$ terms. Intuitively, this estimator can be interpreted as a special case of $\hat{\theta}_{\mathrm{reg}}$ where $k_X(x,x')=\mathbf{1}_{ \{x=x'\}}$. An asymptotic study of $\tilde{\theta}_{\mathrm{reg}}$ is provided by [@alquier2024universal]. The theory and the experiments suggest that both estimators are robust, but that $\hat{\theta}_{\mathrm{reg}}$ is more robust than $\tilde{\theta}_{\mathrm{reg}}$. However, for computations reasons, for large sample sizes $n$ ($n>5\,000$, say) only the latter estimator  can be computed in a reasonable amount of time.

# Package content and implementation {#sec-content}

The package `regMMD` allows to compute the above estimators  in a large number of classical models. We first provide an overview of the two main functions with their default settings: `mmd_reg` for regression, and `mmd_est` for parameter estimation. We then give some details on their implementations and options. These functions have many options related to the choice of kernels, the choice of the bandwidth parameters and of the parameters of the optimization algorithms used to compute the estimators. To save space, in this section we only discuss the options that are fundamental from a statistical perspective. We refer the reader to the package documentation for a full description of the available options.

We start by loading the package and fixing the seed to ensure reproducibility.
```{r r-code0, echo=TRUE}
require("regMMD")
set.seed(1)
```

## Overview of the function `mmd_est`

The function `mmd_est` performs parametric estimation as described in @sec-para. Its required arguments are  the data `x` and the type of model `model` (see @sec-models for the list of available models). Each model implemented in the package has one or two  parameters, namely `par1` and `par2`. If the   model contains a parameter that is fixed  (i.e. not estimated from the data) then its value must be specified by the user. On the other hand, a value for a parameter   that we want to estimate from the data  does not have to be given as an input. If, however, a value is provided then it is used to initialize  the optimization algorithm that serves at computing the estimator (see below). Otherwise  an initialization by default is used.

For example, there are three Gaussian univariate models: `Gaussian.loc`, `Gaussian.scale` and `Gaussian`. In each model `par1` is the mean and `par2` is the standard deviation. We will use the following data:
```{r r-code1, echo=TRUE}
x = rnorm(100,1.9,1)
hist(x)
```

In the `Gaussian` model the two parameters `par1` and `par2` are estimated from the data and the MMD estimator of $\theta=$(`par1`,`par1`) can be computed as follows:

```{r r-code2, echo=TRUE}
estim = mmd_est(x,model="Gaussian")
```

Here, we mention that the output of both `mmd_est` and `mmd_reg` is a list that contains error messages (if any), the estimated parameters (if no error prevented their computations) and various information on the model, the estimators and the algorithms. The major information can be retrived through a `summary` function:
```{r r-code2b, echo=TRUE}
summary(estim)
```
Note that some of the information provided here (like the algorithm used) will be detailed below.

Still in the `Gaussian` model, if we enter
```{r r-code3, echo=TRUE}
estim = mmd_est(x,model="Gaussian",par1=0,par2=1)
summary(estim)
```
we simply enforce the optimization algorithm that serves at computing the estimator to use $\theta_0=(0,1)$ as starting value. In the `Gaussian.loc` model only the location parameter (the mean) is estimated. Thus, to compute $\theta=$ `par1` it is necessary to specify the standard deviation `par2`. For instance,
```{r r-code4, echo=TRUE}
estim = mmd_est(x,model="Gaussian.loc",par2=1)
summary(estim)
```
will estimate $\theta$ in the model $\mathcal{N}(\theta,1)$. If we provide a value for `par1` then the optimization algorithm used to compute $\hat{\theta}$ will use $\theta_0=$`par1` as starting value. Finally, In the `Gaussian.scale` model only the scale parameter (standard deviation) is estimated. That is, to estimate  $\theta=$`par2` in e.g. the $\mathcal{N}(2,\theta^2)$ distribution we can use
```{r r-code5, echo=TRUE}
estim = mmd_est(x,model="Gaussian.scale",par1=2)
summary(estim)
```

## Overview of the function `mmd_reg`

The function `mmd_reg` is used for regression models (@sec-reg)  and requires to specify two arguments, namely the output vector  `y` of size $n$ and the $n\times q$ input matrix `X`.  By default, the functions performs linear regression with Gaussian error noise (with unknown variance) and the regression model to be used can be changed through the option `model` of `mmd_reg` (see @sec-models for the list of available models). In addition, by default, if the input matrix  `X` contains no column whose entries are all equal then an intercept is added to the model, that is, a column of 1's   is added to `X`. This default setting can be disabled by setting the option `intercept` of `mmd_reg` to `FALSE`.

All regression models implemented in the package have a  parameter `par1`, and some of them have an additional scalar parameter `par2`. If a model has `par1` as unique   parameter  then the parameter to be estimated is $\theta=$`par1` and  the conditional distribution of $Y$ given $X=x$ is modelled using a model of the form $(P_{\beta(x^\top\theta)},\theta\in\mathbb{R}^k)$, with $k$ the size of $x$. For instance, for Poisson regression the distribution $P_{\beta(x^\top\theta)}$ is the Poisson distribution with mean $\exp(x^\top\theta)$. By contrast, some  models  have an additional parameter `par2` that also needs to be estimated from the data, so that  $\theta=$(`par1`,`par2`). For these models  the conditional distribution of $Y$ given $X=x$ is modeled using a model of the form $(P_{\beta(x^\top\gamma,\psi)},\theta=(\gamma,\psi)\in\mathbb{R}^{k+1})$. For instance, for the Gaussian linear regression model with unknown variance   $P_{\beta(x^\top\gamma,\psi)}=\mathcal{N}(x^\top\gamma,\psi^2)$. Finally, some  models  have an additional parameter `par2` whose value needs to be specified by the user. For these models $\theta=$`par1`, `par2`$=\psi$ for some $\psi$ fixed by the user, and  the conditional distribution of $Y$ given $X=x$ is modeled using a model of the form $(P_{\beta(x^\top\theta,\psi)},\theta\in\mathbb{R}^{k})$. For instance, for the Gaussian linear regression model with  known variance,  $P_{\beta(x^\top\theta,\psi)}=\mathcal{N}(x^\top\theta,\psi^2)$. Remark that for all models implemented in the package `par1` is   therefore the vector of regression coefficients. As with the function `mmd_est`, if a value  for a parameter that needs to be estimated from the data is provided then it   is used to initialize  the optimization algorithm that serves at computing the estimator, otherwise  an initialization by default is used. It is important to note that the number $k$ of regression coefficients of a model is either $q$ (the    number of columns of the input matrix `X`) or $q+1$  if an intercept has been added by `mmd_reg`. In the latter case, if we want to  provide a value for `par1` then it must be vector of length $q$.
 

For example, there are two linear regression model with Gaussian noise: `linearGaussian` which assumes that noise variance is unknown and `linearGaussian.loc` which assumes that noise variance is unknown. By default, the former model is used by `mmd_reg`, and thus linear regression can be simply performed using `estim = mmd_reg(y,X)`.
By default `mmd_reg` uses the MMD estimator $\tilde{\theta} _{\mathrm{reg}}$, which we recall is cheaper to compute that the alternative MMD estimator $\hat{\theta}_{\mathrm{reg}}$.

## Kernels and bandwidth parameters

For parametric models (Section \ref{sec:Model_fit}) the MMD estimator of $\theta$ is computed with a kernel $k(x,x')$ of the form $k(x,x')=K(\|x-x'\|/\gamma)$  for some  bandwidth parameter $\gamma>0$ and some function $K:[0,\infty)\rightarrow [0,\infty)$. The choice of $K$ and $\gamma$ can be specified through the option  `kernel` and `bdwth` of the function `mmd_est`, respectively.  By default, the median heuristic is used to choose $\gamma$ (the median heuristic was used successfully in many applications of MMD, for example [@gretton2012kernel], see also [@garreau2017large] for a theoretical analysis). The following three options are available for the function  $K$:

* `Gaussian`: $K(u)=\exp(-u^2)$,
* `Laplace`: $K(u)=\exp(-u)$,
* `Cauchy`: $K(u)=1/(2+u^2)$.

Similarly, for regression models (@sec-reg), the MMD estimators are computed with $k_X(x,x')=K_X(\|x-x'\|/\gamma_X)$ and $k_Y(y,y')=K_Y(\|y-y'\|/\gamma_Y)$  for some bandwidth parameters $\gamma_X\geq 0$ and $\gamma_Y>0$ and   functions $K_X, K_Y:[0,\infty)\rightarrow [0,\infty)$. The choice of $K_X$, $K_Y$, and $\gamma_X$ and $\gamma_Y$ can be specified through the option `kernel.x`, `kernel.y`,  `bdwth.x`   and `bdwth.y` of the function `mmd_reg`, respectively. The available choices for $K_X$ and $K_Y$ are the same as for $K$, and by default  the median heuristic is used to select $\gamma_Y$. By default,  `bdwth.x=0` and thus  it is  the  estimator $\tilde{\theta}_{\mathrm{reg}}$ that is used by `mmd_reg`. The alternative estimator $\hat{\theta}_{\mathrm{reg}}$ can be computed either by providing a positive value for `bdwth.x` or by setting `bdwth.x="auto"`, in which case a rescaled version of the  median heuristic is used to choose a positive value for $\gamma_X$.

## Optimization methods

Depending on the model, the  package `regMMD` uses either gradient descent (`GD`)  or stochastic gradient descent (`SGD`) to compute the estimators. 

More precisely, for parametric estimation it is proven in Section 5 of [@cherief2019finite] that the gradient of $\mathbb{D}^2(P_\theta,\hat{P}_n)$ with respect to $\theta$ is given by
$$
  \nabla_\theta \mathbb{D}^2(P_\theta,\hat{P}_n)
 =
 2 \mathbb{E}_{X,X'\sim P_\theta} \left[ \left( k(X,X') - \frac{1}{n}\sum_{i=1}^n k(X_i,X) \right) \nabla_\theta[\log p_\theta(X) ]\right]
$$ {#eq-gradient}
under suitable assumptions on $P_\theta$, including the existence of a density $p_\theta(X)$ and its differentiability. In some models  there is an explicit formula for the expectation in @eq-gradient. This is for instance the case  for the Gaussian mean model, and for such models  a gradient descent algorithm is used to compute the MMD estimator. For models where we cannot compute explicitly the  expectation in @eq-gradient  it is possible to compute an unbiased estimate of the gradient by sampling from $P_\theta$. In this scenario  the MMD estimator is computed  using  AdaGrad [@Duchi], and adaptive step-size SGD algorithm. Finally, in very specific models, $\mathbb{D}(P_\theta,\hat{P}_n)$ can be evaluated explicitly in which  case  we can perform exact optimization `exact`. This is for example  the case when $P_\theta$ is the (discrete) uniform distribution on $\{1,2,\dots,\theta\}$.


In `mmd_est`,  for each model all the available methods for computing the estimator are implemented. The method used by default is chosen according to the ranking: `exact`$>$`GD`$>$`SGD`. We can enforce another method with the `method` option. For instance, `estim <- mmd_est(x,model="Gaussian")` is equivalent to `estim <- mmd_est(x,model="Gaussian",method="GD")`
and we can enforce the use of SGD with `estim <- mmd_est(x,model="Gaussian",method="SGD")`.

For regression models, formulas similar to the one given in @eq-gradient for the gradient of the criteria defining the estimators $\hat{\theta}_{ \mathrm{reg}}$ and $\tilde{\theta}_{ \mathrm{reg}}$ are provided in Section S1 of the supplement of [@alquier2024universal]. For the two estimators, this gradient can be computed explicitly  for all  linear regression models when `kernel.y="Gaussian"`  and for the logistic regression model.

For the  estimator $\tilde{\theta}_{\mathrm{reg}}$, and as for parametric estimation, gradient descent is used when the gradient of the objective function can be computed explicitly,   and otherwise the optimization is performed using Adagrad.  In `mmd_reg` gradient descent is implemented using backtracking line search to select the step-size to use at each iteration, and a stopping rule is implemented to stop the optimization earlier when possible.

The computation  of $\hat{\theta}_{\mathrm{reg}}$ is more delicate. Indeed, the objective function defining this estimator is the sum of $n^2$ terms (see @sec-reg), implying that  minimizing this function using     gradient descent or SGD leads to algorithms for computing the estimator that require $\mathcal{O}(n^2)$ operations per iteration. In the package, to reduce the cost per iteration we implement the strategy proposed in Section S1 of the supplement of [@alquier2024universal]. Importantly, with this strategy the optimization is performed using an unbiased estimate of the gradient of the objective function, even when the gradient  of the $n^2$ terms of objective function can be computed explicitly. It is  however possible to use the explicit formula for these gradients   to reduce the variance of the noisy gradient, which we do   when possible. As for the computation of $\tilde{\theta}_{\mathrm{reg}}$ a stopping rule is implemented to stop the optimization earlier when possible. With this package it is feasible to compute $\hat{\theta}_{\mathrm{reg}}$ in a reasonable amount of time for dataset containing up to a few thousands data points (up-to $n\approx 5\,000$ observations, say). 


Finally, we stress that, from a computational point of view,    MMD estimation in regression models  is a much more challenging task than in parametric models for at least two reasons. Firstly, while in the latter task the dimension of the parameter of interest $\theta$ is at most two for the models implemented in this package (see below), in regression the dimension of $\theta$ can be much larger, depending on the number of explanatory variables. Secondly, the objective functions to minimize for regression models are ''more non-linear''. When estimating a regression model it is therefore a good practice to verify that the optimization of the objective function has converged. This can be done by inspecting the sequence of $\theta$ values computed by the optimization algorithm, accessible from the  object `trace` of `mmd_reg`.

## Available models {#sec-models}

List of univariate models in `mmd_est`:

* Gaussian $\mathcal{N}(m,\sigma^2)$: `Gaussian`(estimation of $m$ and $\sigma$), `Gaussian.loc`(estimation of $m$) and `Gaussian.scale`(estimation of $\sigma$),
* Cauchy: `Cauchy`(estimation of the location parameter),
* Pareto: `Pareto`(estimation of the exponent),
* exponential $\mathcal{E}(\lambda)$: `exponential},
* gamma $Gamma(a,b)$: `gamma`(estimation of $a$ and $b$), `gamma.shape`(estimation of $a$) and `gamma.rate`(estimation of $b$),
* continuous uniform: `continuous.uniform.loc`(estimation of $m$ in $\mathcal{U}[m-\frac{L}{2},m+\frac{L}{2}]$, where $L$ is fixed by the user), `continuous.uniform.upper`(estimation of $b$ in $\mathcal{U}[a,b]$) and `continuous.uniform.lower.upper`(estimation of both $a$ and $b$ in $\mathcal{U}[a,b]$),
* Dirac $\delta_a$: `Dirac`(estimation of $a$; while this might sound uninteresting at first, this can be used to define a ''model-free'' robust location parameter),
* discrete uniform $\mathcal{U}(\{1,2,\dots,N\})$: `discrete.uniform`(estimation of $N$),
* binomial $Bin(N,p)$: `binomial`(estimation of $N$ and $p$), `binomial.size`(estimation of $N$) and `binomial.prob`(estimation of $p$),
* geometric $\mathcal{G}(p)$: `geometric`,
* Poisson $\mathcal{P}(\lambda)$: `Poisson`.

List of multivariate models in `mmd_est`:

* multivariate Gaussian $\mathcal{N}(\mu,U U^T)$: `multidim.Gaussian` (estimation of $\mu$ and $U$), `multidim.Gaussian.loc` (estimation of $\mu$ while $U=\sigma I$ for a fixed $\sigma>0$) and `multidim.Gaussian.scale` (estimation of $U$ while $\mu$ is fixed),
* Dirac mass $\delta_{a}$: `multidim.Dirac`.

List of regression models in `mmd_reg`:

* linear regression models with Gaussian noise: `linearGaussian` (unknown noise variance) and `linearGaussian.loc` (known noise variance),
* exponential regression: `exponential`,
* gamma regression: `gamma`, or `gamma.loc` when the precision parameter is known,
* beta regression: `beta`, or `beta.loc` when the precision parameter is known,
* logistic regression: `logistic`,
* Poisson regression: `poisson`.



# References {.unnumbered}

::: {#refs}
:::

# Session information {.appendix .unnumbered}

```{r session-info}
sessionInfo()
```

