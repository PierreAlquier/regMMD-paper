# Introduction

In some models, popular estimators such as the maximum likelihood estimator (MLE) can become very unstable in the presence of outliers. This has motivated research into robust estimation procedures that would not suffer from this issue. Various notions of robustness have been proposed, depending on the type of contamination in the data. Notably, the Huber contamination model [@huber1964robust] considers random outliers while, more recently, stricter notions have been proposed to ensure robustness against adversarial contamination of the data.

The maximum mean discrepancy (MMD) is a kernel-based metric that has received considerable attention in the past 15 years. It allows for the development of tools for nonparametric tests and estimation [@pmlr-v48-chwialkowski16;@gretton2007kernel]. We refer the reader to [@muandet2017kernel] for a comprehensive introduction to MMD and its applications. A recent series of papers has suggested that minimum distance estimators (MDEs) based on the MMD are robust to both Huber and adversarial contamination. These estimators were initially proposed for training generative AI [@dziugaite2015training;@sutherland2016generative;@li2017mmd], and the study of their statistical properties for parametric estimation, including robustness, was initiated by [@briol2019statistical;@cherief2019finite;@alquier2024universal]. We also point out that the MMD has been successfully used to define robust estimators that are not MDEs, such as bootstrap methods based on MMD [@dellaporta2022robust] or Approximate Bayesian Computation [@legramanti2022concentration].

Unfortunately, we are not aware of any software that allows for the computation of MMD-based MDEs. To make these tools available to the statistical community, we developed the `R` package called `regMMD`. This package allows for the minimization of the MMD distance between the empirical distribution and a statistical model. Various parametric models can be fitted, including continuous distributions such as Gaussian and gamma, and discrete distributions such as Poisson and binomial. Many regression models are also available, including linear, logistic, and gamma regression. The `regMMD` package is available on the `CRAN` website [@R-base]: [regMMD page](https://cran.r-project.org/web/packages/regMMD/)

The optimization is based on the strategies proposed by [@briol2019statistical;@cherief2019finite;@alquier2024universal]. For some models  we have an explicit formula for the gradient of the MMD, in which   case  we   use gradient descent, see e.g. [@boyd2004convex;@nesterov2018lectures] to perform the optimization. For most models  such a formula does not exist, but we can however   approximate the gradient without bias by Monte Carlo sampling. This allows to use the stochastic gradient algorithm of [@robbins1951stochastic], that is one of the most popular estimation methods in machine learning [@bottou-mlss-2004]. We refer to the reader to  [@wright2018optimization;@duchi2018introductory] and Chapter 5 in [@bach2024learning] for comprehensive introductions to otpimization for statistics and machine learning, including stochastic optimization methods.

The paper is organized as follows. In @sec-stat we briefly recall the construction of the MMD metric and the MDE estimators based on the MMD. In @sec-content  we detail the content of the package `regMMD`: the available models and kernels, and the optimization procedures used in each case. Finally, in @sec-examples we provide examples of applications of `regMMD`. Note that these experiments are not meant to be a comprehensive comparison of MMD to other robust estimation procedures. Exhaustive comparisons can be found in [@briol2019statistical;@cherief2019finite;@alquier2024universal]. The objective is simply to illustrate the use of `regMMD` through pedagogical examples.

# Statistical background {#sec-stat}

## Parametric estimation {#sec-para}

Let $X_1,\dots,X_n$ be $\mathcal{X}$-valued random variables identically distributed according to some probability distribution $P^0$, and let $(P_\theta,\theta\in\Theta)$ be a statistical model. Given a metric $d$ on probability distributions, we are looking for an estimator of $\theta_0 \in\arg\min_{\theta\in\Theta} d(P_\theta,P^0)$ when such a minimum exists. Letting   $\hat{P}_n=\frac{1}{n}\sum_{i=1}^n \delta_{X_i}$ denote the empirical probability distribution, the  minimum distance estimator (MDE) $\hat{\theta}$ is defined as follows:
$$
\hat{\theta} \in \arg\min_{\theta\in\Theta} d(P_\theta,\hat{P}_n ).
$$
The robustness properties of MDEs for well chosen distances was studied as early as in [@wolfowitz1957minimum;@parr1980minimum;@yatracos2022limitations]. When $d(P_\theta,\hat{P}_n )$ has no minimum, the definition can be replaced by an $\varepsilon$-approximate minimizer, without consequences on the the properties of the estimator, as shown in to [@briol2019statistical;@cherief2019finite].

Let $\mathcal{H}$ be a Hilbert space, let $\|\cdot\|_{\mathcal{H}}$ and $\left<\cdot,\cdot\right>_{\mathcal{H}}$  denote the associated norms and scalar products, respectively, and let $\varphi:\mathcal{X}\rightarrow\mathcal{H}$. Then, for any probability distribution $P$ on $\mathcal{X}$ such that $\mathbb{E}_{X\sim P}[\|\varphi(X)\|_{\mathcal{H}}]<+\infty$ we can define the mean embedding $\mu(P)=\mathbb{E}_{X\sim P}[\varphi(X)]$. When the   mean embedding $\mu(P)$ is defined for any probability distribution $P$ (e.g. because the map $\varphi$ is bounded in $\mathcal{H}$), for any probability distributions $P$ and $Q$ we put
$$
\mathbb{D}(P,Q) := \left\| \mu(P) - \mu(Q) \right\|_{\mathcal{H}}.
$$
Letting $k(x,y)=\left<\varphi(x),\varphi(y)\right>_{\mathcal{H}}$, it appears that $\mathbb{D}(P,Q)$ depends on $\varphi$ only through $k$, as we can rewrite:
$$
\mathbb{D}^2(P,Q)
= \mathbb{E}_{X,X'\sim P} [k(X,X')] -2 \mathbb{E}_{X\sim P,X'\sim Q} [k(X,X')] + \mathbb{E}_{X,X'\sim Q} [k(X,X')].
$$ {#eq-MMD}
When $\mathcal{H}$ is actually a RKHS for the kernel $k$ (see [@muandet2017kernel] for a definition), $\mathbb{D}(P,Q)$ is called the maximum mean discrepancy (MMD) between $P$ and $Q$. A condition on $k$ (universal kernel) ensures that the map $\mu$ is injective, and thus that $\mathbb{D}$ satisfies the axioms of a metric. Examples of universal kernels are known, such as the Gaussian kernel $k(x,y)=\exp(-\|x-y\|^2/\gamma^2)$ or the Laplace kernel $k(x,y)=\exp(-\|x-y\|/\gamma)$, see [@muandet2017kernel] for more examples and references to the proofs.

The properties of MDEs based on MMD were studied in [@briol2019statistical;@cherief2019finite]. In particular, when the kernel $k$ is bounded, this estimator enjoys very strong robustness properties. We cite the following very simple result.

::: {#thm-cherief}

## special case of Theorem 3.1 in [@cherief2019finite]

Assume $X_1,\dots,X_n$ are i.i.d. from $P^0$. Assume the kernel $k$ is bounded by $1$. Then
$$
\mathbb{E} \left[ \mathbb{D}\left( P_{\hat{\theta}}, P^0 \right) \right]
\leq \inf_{\theta\in\Theta} \mathbb{D}\left( P_{\theta}, P^0 \right) + \frac{2}{\sqrt{n}}.
$$
:::

Additional non-asymptotic results can be found in [@briol2019statistical;@cherief2019finite]. In particular, Theorem 3.1 of [@cherief2019finite] also covers non independent observations (time series). An asymptotic study of $\hat{\theta}$, including conditions for asymptotic normality, can be found in [@briol2019statistical]. All these works provide strong theoretical evidence that $\hat{\theta}$ is very robust to random and adversarial contamination of the data, and this is supported by empirical evidence.

## Regression {#sec-reg}

Let us now consider a regression setting: we observe $(X_1,Y_1),\dots,(X_n,Y_n)$ in $\mathcal{X}\times \mathcal{Y}$ and we want to estimate the conditional distribution $P^0_{Y|X=x}$ of $Y$ given $X=x$ for any $x$. %A direct application of the method in the previous section to the random variables $(X_1,Y_1),\dots, (X_n,Y_n)$ would   lead to the estimation of the joint distribution of the pair $(X,Y)$, which is not the objective of regression models.
To this end we consider a statistical model $(P_\beta,\beta\in\mathcal{B})$ and model the conditional distribution of $Y$ given $X=x$ by $(P_{\beta(x,\theta)},\theta\in\Theta)$ where $\beta(\cdot,\cdot)$ is a specified function $\mathcal{X}\times\Theta \rightarrow \mathcal{B}$. The first estimator proposed by [@alquier2024universal] is:
$$
\hat{\theta}_{\mathrm{reg}}\in\arg\min_{\theta\in\Theta} \mathbb{D}\left(
\frac{1}{n}\sum_{i=1}^n \delta_{X_i} \otimes P_{\beta(X_i,\theta)},
\frac{1}{n}\sum_{i=1}^n \delta_{X_i} \otimes \delta_{Y_i}
\right)
$$
where $\mathbb{D}$ is the MMD defined by a product kernel, that is  a kernel of the form $k((x,y),(x',y'))=k_X(x,x')k_Y(y,y')$ (non-product kernels are theoretically possible, but not implemented in the package). Asymptotic and non-asymptotic properties of $\hat{\theta}$ are studied in [@alquier2024universal]. The computation of $\hat{\theta}$ is however slow when the sample size $n$ is large, as it can be shown that  the criterion defining this estimator is the sum of $n^2$ terms.

By contrast, the following alternative estimator 
$$
\tilde{\theta}_{\mathrm{reg}}\in\arg\min_{\theta\in\Theta} \frac{1}{n}\sum_{i=1}^n \mathbb{D}\left(
\delta_{X_i} \otimes P_{\beta(X_i,\theta)},
\delta_{X_i} \otimes \delta_{Y_i}
\right),
$$
has the advantage to be defined through a criterion which is a sum of only $n$ terms. Intuitively, this estimator can be interpreted as a special case of $\hat{\theta}_{\mathrm{reg}}$ where $k_X(x,x')=\mathbf{1}_{ \{x=x'\}}$. An asymptotic study of $\tilde{\theta}_{\mathrm{reg}}$ is provided by [@alquier2024universal]. The theory and the experiments suggest that both estimators are robust, but that $\hat{\theta}_{\mathrm{reg}}$ is more robust than $\tilde{\theta}_{\mathrm{reg}}$. However, for computations reasons, for large sample sizes $n$ ($n>5\,000$, say) only the latter estimator  can be computed in a reasonable amount of time.

# Package content and implementation {#sec-content}

The package `regMMD` allows to compute the above estimators  in a large number of classical models. We first provide an overview of the two main functions with their default settings: `mmd_reg` for regression, and `mmd_est` for parameter estimation. We then give some details on their implementations and options. These functions have many options related to the choice of kernels, the choice of the bandwidth parameters and of the parameters of the optimization algorithms used to compute the estimators. To save space, in this section we only discuss the options that are fundamental from a statistical perspective. We refer the reader to the package documentation for a full description of the available options.

We start by loading the package and fixing the seed to ensure reproducibility.
```{r r-code0, echo=TRUE}
require("regMMD")
set.seed(1)
```

## Overview of the function `mmd_est`

The function `mmd_est` performs parametric estimation as described in @sec-para. Its required arguments are  the data `x` and the type of model `model` (see @sec-models for the list of available models). Each model implemented in the package has one or two  parameters, namely `par1` and `par2`. If the   model contains a parameter that is fixed  (i.e. not estimated from the data) then its value must be specified by the user. On the other hand, a value for a parameter   that we want to estimate from the data  does not have to be given as an input. If, however, a value is provided then it is used to initialize  the optimization algorithm that serves at computing the estimator (see below). Otherwise  an initialization by default is used.

For example, there are three Gaussian univariate models: `Gaussian.loc`, `Gaussian.scale` and `Gaussian`. In each model `par1` is the mean and `par2` is the standard deviation. We will use the following data:
```{r r-code1, echo=TRUE}
x = rnorm(100,1.9,1)
hist(x)
```

In the `Gaussian` model the two parameters `par1` and `par2` are estimated from the data and the MMD estimator of $\theta=$(`par1`,`par1`) can be computed as follows:

```{r r-code2, echo=TRUE}
estim = mmd_est(x,model="Gaussian")
```

Here, we mention that the output of both `mmd_est` and `mmd_reg` is a list that contains error messages (if any), the estimated parameters (if no error prevented their computations) and various information on the model, the estimators and the algorithms. The major information can be retrived through a `summary` function:
```{r r-code2b, echo=TRUE}
summary(estim)
```
Note that some of the information provided here (like the algorithm used) will be detailed below.

Still in the `Gaussian` model, if we enter
```{r r-code3, echo=TRUE}
estim = mmd_est(x,model="Gaussian",par1=0,par2=1)
summary(estim)
```
we simply enforce the optimization algorithm that serves at computing the estimator to use $\theta_0=(0,1)$ as starting value. In the `Gaussian.loc` model only the location parameter (the mean) is estimated. Thus, to compute $\theta=$ `par1` it is necessary to specify the standard deviation `par2`. For instance,
```{r r-code4, echo=TRUE}
estim = mmd_est(x,model="Gaussian.loc",par2=1)
summary(estim)
```
will estimate $\theta$ in the model $\mathcal{N}(\theta,1)$. If we provide a value for `par1` then the optimization algorithm used to compute $\hat{\theta}$ will use $\theta_0=$`par1` as starting value. Finally, In the `Gaussian.scale` model only the scale parameter (standard deviation) is estimated. That is, to estimate  $\theta=$`par2` in e.g. the $\mathcal{N}(2,\theta^2)$ distribution we can use
```{r r-code5, echo=TRUE}
estim = mmd_est(x,model="Gaussian.scale",par1=2)
summary(estim)
```

## Overview of the function `mmd_reg`

The function `mmd_reg` is used for regression models (@sec-reg)  and requires to specify two arguments, namely the output vector  `y` of size $n$ and the $n\times q$ input matrix `X`.  By default, the functions performs linear regression with Gaussian error noise (with unknown variance) and the regression model to be used can be changed through the option `model` of `mmd_reg` (see @sec-models for the list of available models). In addition, by default, if the input matrix  `X` contains no column whose entries are all equal then an intercept is added to the model, that is, a column of 1's   is added to `X`. This default setting can be disabled by setting the option `intercept` of `mmd_reg` to `FALSE`.

All regression models implemented in the package have a  parameter `par1`, and some of them have an additional scalar parameter `par2`. If a model has `par1` as unique   parameter  then the parameter to be estimated is $\theta=$`par1` and  the conditional distribution of $Y$ given $X=x$ is modelled using a model of the form $(P_{\beta(x^\top\theta)},\theta\in\mathbb{R}^k)$, with $k$ the size of $x$. For instance, for Poisson regression the distribution $P_{\beta(x^\top\theta)}$ is the Poisson distribution with mean $\exp(x^\top\theta)$. By contrast, some  models  have an additional parameter `par2` that also needs to be estimated from the data, so that  $\theta=$(`par1`,`par2`). For these models  the conditional distribution of $Y$ given $X=x$ is modeled using a model of the form $(P_{\beta(x^\top\gamma,\psi)},\theta=(\gamma,\psi)\in\mathbb{R}^{k+1})$. For instance, for the Gaussian linear regression model with unknown variance   $P_{\beta(x^\top\gamma,\psi)}=\mathcal{N}(x^\top\gamma,\psi^2)$. Finally, some  models  have an additional parameter `par2` whose value needs to be specified by the user. For these models $\theta=$`par1`, `par2`$=\psi$ for some $\psi$ fixed by the user, and  the conditional distribution of $Y$ given $X=x$ is modeled using a model of the form $(P_{\beta(x^\top\theta,\psi)},\theta\in\mathbb{R}^{k})$. For instance, for the Gaussian linear regression model with  known variance,  $P_{\beta(x^\top\theta,\psi)}=\mathcal{N}(x^\top\theta,\psi^2)$. Remark that for all models implemented in the package `par1` is   therefore the vector of regression coefficients. As with the function `mmd_est`, if a value  for a parameter that needs to be estimated from the data is provided then it   is used to initialize  the optimization algorithm that serves at computing the estimator, otherwise  an initialization by default is used. It is important to note that the number $k$ of regression coefficients of a model is either $q$ (the    number of columns of the input matrix `X`) or $q+1$  if an intercept has been added by `mmd_reg`. In the latter case, if we want to  provide a value for `par1` then it must be vector of length $q$.
 

For example, there are two linear regression model with Gaussian noise: `linearGaussian` which assumes that noise variance is unknown and `linearGaussian.loc` which assumes that noise variance is unknown. By default, the former model is used by `mmd_reg`, and thus linear regression can be simply performed using `estim = mmd_reg(y,X)`.
By default `mmd_reg` uses the MMD estimator $\tilde{\theta} _{\mathrm{reg}}$, which we recall is cheaper to compute that the alternative MMD estimator $\hat{\theta}_{\mathrm{reg}}$.

## Kernels and bandwidth parameters

For parametric models (Section \ref{sec:Model_fit}) the MMD estimator of $\theta$ is computed with a kernel $k(x,x')$ of the form $k(x,x')=K(\|x-x'\|/\gamma)$  for some  bandwidth parameter $\gamma>0$ and some function $K:[0,\infty)\rightarrow [0,\infty)$. The choice of $K$ and $\gamma$ can be specified through the option  `kernel` and `bdwth` of the function `mmd_est`, respectively.  By default, the median heuristic is used to choose $\gamma$ (the median heuristic was used successfully in many applications of MMD, for example [@gretton2012kernel], see also [@garreau2017large] for a theoretical analysis). The following three options are available for the function  $K$:

* `Gaussian`: $K(u)=\exp(-u^2)$,
* `Laplace`: $K(u)=\exp(-u)$,
* `Cauchy`: $K(u)=1/(2+u^2)$.

Similarly, for regression models (@sec-reg), the MMD estimators are computed with $k_X(x,x')=K_X(\|x-x'\|/\gamma_X)$ and $k_Y(y,y')=K_Y(\|y-y'\|/\gamma_Y)$  for some bandwidth parameters $\gamma_X\geq 0$ and $\gamma_Y>0$ and   functions $K_X, K_Y:[0,\infty)\rightarrow [0,\infty)$. The choice of $K_X$, $K_Y$, and $\gamma_X$ and $\gamma_Y$ can be specified through the option `kernel.x`, `kernel.y`,  `bdwth.x`   and `bdwth.y` of the function `mmd_reg`, respectively. The available choices for $K_X$ and $K_Y$ are the same as for $K$, and by default  the median heuristic is used to select $\gamma_Y$. By default,  `bdwth.x=0` and thus  it is  the  estimator $\tilde{\theta}_{\mathrm{reg}}$ that is used by `mmd_reg`. The alternative estimator $\hat{\theta}_{\mathrm{reg}}$ can be computed either by providing a positive value for `bdwth.x` or by setting `bdwth.x="auto"`, in which case a rescaled version of the  median heuristic is used to choose a positive value for $\gamma_X$.

## Optimization methods

Depending on the model, the  package `regMMD` uses either gradient descent (`GD`)  or stochastic gradient descent (`SGD`) to compute the estimators. 

More precisely, for parametric estimation it is proven in Section 5 of [@cherief2019finite] that the gradient of $\mathbb{D}^2(P_\theta,\hat{P}_n)$ with respect to $\theta$ is given by
$$
  \nabla_\theta \mathbb{D}^2(P_\theta,\hat{P}_n)
 =
 2 \mathbb{E}_{X,X'\sim P_\theta} \left[ \left( k(X,X') - \frac{1}{n}\sum_{i=1}^n k(X_i,X) \right) \nabla_\theta[\log p_\theta(X) ]\right]
$$ {#eq-gradient}
under suitable assumptions on $P_\theta$, including the existence of a density $p_\theta(X)$ and its differentiability. In some models  there is an explicit formula for the expectation in @eq-gradient. This is for instance the case  for the Gaussian mean model, and for such models  a gradient descent algorithm is used to compute the MMD estimator. For models where we cannot compute explicitly the  expectation in @eq-gradient  it is possible to compute an unbiased estimate of the gradient by sampling from $P_\theta$. In this scenario  the MMD estimator is computed  using  AdaGrad [@Duchi], and adaptive step-size SGD algorithm. Finally, in very specific models, $\mathbb{D}(P_\theta,\hat{P}_n)$ can be evaluated explicitly in which  case  we can perform exact optimization `exact`. This is for example  the case when $P_\theta$ is the (discrete) uniform distribution on $\{1,2,\dots,\theta\}$.


In `mmd_est`,  for each model all the available methods for computing the estimator are implemented. The method used by default is chosen according to the ranking: `exact`$>$`GD`$>$`SGD`. We can enforce another method with the `method` option. For instance, `estim <- mmd_est(x,model="Gaussian")` is equivalent to `estim <- mmd_est(x,model="Gaussian",method="GD")`
and we can enforce the use of SGD with `estim <- mmd_est(x,model="Gaussian",method="SGD")`.

For regression models, formulas similar to the one given in @eq-gradient for the gradient of the criteria defining the estimators $\hat{\theta}_{ \mathrm{reg}}$ and $\tilde{\theta}_{ \mathrm{reg}}$ are provided in Section S1 of the supplement of [@alquier2024universal]. For the two estimators, this gradient can be computed explicitly  for all  linear regression models when `kernel.y="Gaussian"`  and for the logistic regression model.

For the  estimator $\tilde{\theta}_{\mathrm{reg}}$, and as for parametric estimation, gradient descent is used when the gradient of the objective function can be computed explicitly,   and otherwise the optimization is performed using Adagrad.  In `mmd_reg` gradient descent is implemented using backtracking line search to select the step-size to use at each iteration, and a stopping rule is implemented to stop the optimization earlier when possible.

The computation  of $\hat{\theta}_{\mathrm{reg}}$ is more delicate. Indeed, the objective function defining this estimator is the sum of $n^2$ terms (see @sec-reg), implying that  minimizing this function using     gradient descent or SGD leads to algorithms for computing the estimator that require $\mathcal{O}(n^2)$ operations per iteration. In the package, to reduce the cost per iteration we implement the strategy proposed in Section S1 of the supplement of [@alquier2024universal]. Importantly, with this strategy the optimization is performed using an unbiased estimate of the gradient of the objective function, even when the gradient  of the $n^2$ terms of objective function can be computed explicitly. It is  however possible to use the explicit formula for these gradients   to reduce the variance of the noisy gradient, which we do   when possible. As for the computation of $\tilde{\theta}_{\mathrm{reg}}$ a stopping rule is implemented to stop the optimization earlier when possible. With this package it is feasible to compute $\hat{\theta}_{\mathrm{reg}}$ in a reasonable amount of time for dataset containing up to a few thousands data points (up-to $n\approx 5\,000$ observations, say). 


Finally, we stress that, from a computational point of view,    MMD estimation in regression models  is a much more challenging task than in parametric models for at least two reasons. Firstly, while in the latter task the dimension of the parameter of interest $\theta$ is at most two for the models implemented in this package (see below), in regression the dimension of $\theta$ can be much larger, depending on the number of explanatory variables. Secondly, the objective functions to minimize for regression models are ''more non-linear''. When estimating a regression model it is therefore a good practice to verify that the optimization of the objective function has converged. This can be done by inspecting the sequence of $\theta$ values computed by the optimization algorithm, accessible from the  object `trace` of `mmd_reg`.

## Available models {#sec-models}

List of univariate models in `mmd_est`:

* Gaussian $\mathcal{N}(m,\sigma^2)$: `Gaussian`(estimation of $m$ and $\sigma$), `Gaussian.loc`(estimation of $m$) and `Gaussian.scale`(estimation of $\sigma$),
* Cauchy: `Cauchy`(estimation of the location parameter),
* Pareto: `Pareto`(estimation of the exponent),
* exponential $\mathcal{E}(\lambda)$: `exponential},
* gamma $Gamma(a,b)$: `gamma`(estimation of $a$ and $b$), `gamma.shape`(estimation of $a$) and `gamma.rate`(estimation of $b$),
* continuous uniform: `continuous.uniform.loc`(estimation of $m$ in $\mathcal{U}[m-\frac{L}{2},m+\frac{L}{2}]$, where $L$ is fixed by the user), `continuous.uniform.upper`(estimation of $b$ in $\mathcal{U}[a,b]$) and `continuous.uniform.lower.upper`(estimation of both $a$ and $b$ in $\mathcal{U}[a,b]$),
* Dirac $\delta_a$: `Dirac`(estimation of $a$; while this might sound uninteresting at first, this can be used to define a ''model-free'' robust location parameter),
* discrete uniform $\mathcal{U}(\{1,2,\dots,N\})$: `discrete.uniform`(estimation of $N$),
* binomial $Bin(N,p)$: `binomial`(estimation of $N$ and $p$), `binomial.size`(estimation of $N$) and `binomial.prob`(estimation of $p$),
* geometric $\mathcal{G}(p)$: `geometric`,
* Poisson $\mathcal{P}(\lambda)$: `Poisson`.

List of multivariate models in `mmd_est`:

* multivariate Gaussian $\mathcal{N}(\mu,U U^T)$: `multidim.Gaussian` (estimation of $\mu$ and $U$), `multidim.Gaussian.loc` (estimation of $\mu$ while $U=\sigma I$ for a fixed $\sigma>0$) and `multidim.Gaussian.scale` (estimation of $U$ while $\mu$ is fixed),
* Dirac mass $\delta_{a}$: `multidim.Dirac`.

List of regression models in `mmd_reg`:

* linear regression models with Gaussian noise: `linearGaussian` (unknown noise variance) and `linearGaussian.loc` (known noise variance),
* exponential regression: `exponential`,
* gamma regression: `gamma`, or `gamma.loc` when the precision parameter is known,
* beta regression: `beta`, or `beta.loc` when the precision parameter is known,
* logistic regression: `logistic`,
* Poisson regression: `poisson`.

# Detailed examples {#sec-examples}

## Toy example: robust estimation in the univariate Gaussian model

We start with a very simple illustration on synthetic data. We choose one of the simplest model, namely, estimation of the mean of a univariate Gaussian random variable. The statistical model is $\mathcal{N}(\theta,1)$, which is the `Gaussian.loc` model in the package. We remind the above, using the default settings:
```{r r-code6, echo=TRUE}
estim = mmd_est(x,model="Gaussian.loc",par2=1)
summary(estim)
```
The user can also impose a different bandwidth and kernel, which will result in a different estimator:
```{r r-code7, echo=TRUE}
estim =  mmd_est(x,model="Gaussian.loc",par2=1,bdwth=0.6,kernel="Laplace")
summary(estim)
```

We end up the discussion on the Gaussian mean example by a toy experiment: we replicate $N=200$ times the simulation of $n=100$ i.i.d. random variables $\mathcal{N}(-2,1)$ and compare the maximum likelihood estimator (MLE), equal to the empirical mean, the median, and the MMD estimator with a Gaussian and with a Laplace kernel, using in both cases the median heuristic for the bandwidth parameter. We report the Mean Absolute Error over all the simulations (MAE) as well as the standard deviation of the Absolute Error (sdAE).
```{r r-code8, echo=TRUE}
N = 10 # 200
n = 100
theta = -2
err = matrix(data=0,nr=N,nc=4)
for (i in 1:N)
{
	X = rnorm(n,theta,1)
	thetamle = mean(X)
	thetamed = median(X)
	estim = mmd_est(X,model="Gaussian.loc",par2=1)
	thetanew1 = estim$estimator
	estim = mmd_est(X,model="Gaussian.loc",par2=1,kernel="Laplace")
	thetanew2 = estim$estimator
	err[i,] = abs(theta-c(thetamle,thetanew1,thetanew2,thetamed))
}
results = matrix(0,nr=2,nc=4)
results[1,] = colSums(err)/N
for (i in 1:4) results[2,i] = sd(err[,i])
colnames(results) = c('MLE','MMD (Gaussian kernel)','MMD (Laplace kernel)','Median')
rownames(results) = c('MAE','sdAE')
final = as.table(results)
knitr::kable(final,caption="Estimation of the mean in the uncontaminated case")
```
In a second time, we repeat the same experiment with contamination: two of the $X_i$'s are sampled from a standard Cauchy distribution instead.
```{r r-code9, echo=TRUE}
N = 10 # 200
n = 100
theta = -2
err = matrix(data=0,nr=N,nc=4)
for (i in 1:N)
{
	cont = rcauchy(2)
	X = c(rnorm(n-2,theta,1),cont)
	thetamle = mean(X)
	thetamed = median(X)
	estim = mmd_est(X,model="Gaussian.loc",par2=1)
	thetanew1 = estim$estimator
	estim = mmd_est(X,model="Gaussian.loc",par2=1,kernel="Laplace")
	thetanew2 = estim$estimator
	err[i,] = abs(theta-c(thetamle,thetanew1,thetanew2,thetamed))
}
results = matrix(0,nr=2,nc=4)
results[1,] = colSums(err)/N
for (i in 1:4) results[2,i] = sd(err[,i])
colnames(results) = c('MLE','MMD (Gaussian kernel)','MMD (Laplace kernel)','Median')
rownames(results) = c('MAE','sdAE')
final = as.table(results)
knitr::kable(final,caption="Estimation of the mean in the contaminated case")
```
The results are as expected: under no contamination, the MLE is known to be efficient and is therefore the best estimator. On the other hand, the MLE (i.e.~the empirical mean) is known to be very sensitive to outliers. In contrast,  the    MMD estimators and the median are robust estimators of $\theta$. We refer the reader to [@briol2019statistical;@cherief2019finite] for more discussions and experiments on the robustness   MMD estimators.
 
Note that while the median is a natural robust alternative to the MLE in the Gaussian mean model, such an alternative is not always available. Consider for example the estimation of the standard deviation of a Gaussian, with known zero mean: $\mathcal{N}(0,\theta^2)$. We cannot use (directly) a median in this case, while the MMD estimators are available. We repeat similar experiments as above in this model. We let $N$ and $n$ be as above and sample  the uncontaminated   observations from the $\mathcal{N}(0,1)$ distributions. We them compare the MLE to the MMD with Gaussian and Laplace kernel both in the uncontaminated case  and under Cauchy contamination for two observations. The conclusions are completely similar to the ones obtained in the Gaussian location experiments.
```{r r-code10, echo=TRUE}
N = 10 # 200
n = 100
theta=1
err = matrix(data=0,nr=N,nc=3)
for (i in 1:N)
{
	X = rnorm(n,0,theta)
	thetamle = sqrt(mean(X^2))
	estim = mmd_est(X,model="Gaussian.scale",par1=0)
	thetanew1 = estim$estimator
	estim = mmd_est(X,model="Gaussian.scale",par1=0,kernel="Laplace")
	thetanew2 = estim$estimator
	err[i,] = abs(theta-c(thetamle,thetanew1,thetanew2))
}
results = matrix(0,nr=2,nc=3)
results[1,] = colSums(err)/N
for (i in 1:3) results[2,i] = sd(err[,i])
colnames(results) = c('MLE','MMD (Gaussian kernel)','MMD (Laplace kernel)')
rownames(results) = c('MAE','sdAE')
final = as.table(results)
knitr::kable(final,caption="Estimation of the standard-deviation in the uncontaminated case")
```
```{r r-code11, echo=TRUE}
N = 10 # 200
n = 100
theta=1
err = matrix(data=0,nr=N,nc=3)
for (i in 1:N)
{
	cont = rcauchy(2)
	X = c(rnorm(n-2,0,theta),cont)
	thetamle = sqrt(mean(X^2))
	estim = mmd_est(X,model="Gaussian.scale",par1=0)
	thetanew1 = estim$estimator
	estim = mmd_est(X,model="Gaussian.scale",par1=0,kernel="Laplace")
	thetanew2 = estim$estimator
	err[i,] = abs(theta-c(thetamle,thetanew1,thetanew2))
}
results = matrix(0,nr=2,nc=3)
results[1,] = colSums(err)/N
for (i in 1:3) results[2,i] = sd(err[,i])
colnames(results) = c('MLE','MMD (Gaussian kernel)','MMD (Laplace kernel)')
rownames(results) = c('MAE','sdAE')
final = as.table(results)
knitr::kable(final,caption="Estimation of the standard-deviation in the contaminated case")
```

## Robust linear regression

### Dataset and model

To illustrate the use of the   {\sc \textbf{regMMD}}  package to perform robust linear regression we use the \textsf{R} built-in dataset \texttt{airquality}. The dataset contains daily measurements of four variables related to air quality in New York, namely the ozone concentration (variable \texttt{Ozone}), the temperature (variable \texttt{Temp}), the wind speed (variable \texttt{Wind}) and the solar radiation (variable \texttt{Solar.R}). Observations are reported for the period ranging from the 1st of May 1973 to the 30th of September 1973, resulting in a total of  153 observations.  The dataset contains 42 observations with missing values, which we remove from the sample.







\begin{figure}[!t]
\centering
\includegraphics[scale=0.4]{figs/ylinreg.pdf}
\hspace{0.2cm}\includegraphics[scale=0.4]{figs/ypoissreg.pdf}
\caption{\label{figs:ozone1} Distribution of the observed values of $\log(\text{\texttt{Ozone}})$ (left) and of the observed values of \texttt{Ozone} (right).}
\end{figure}

 


Our aim is to study the link between the level of ozone concentration and the other three variables present in the dataset using the following linear regression model:
\begin{equation*}
\begin{split}
\log(\text{\texttt{Ozone}}) =\alpha&+\beta_1\text{\texttt{Temp}}+\beta_2(\text{\texttt{Temp}})^2+\beta_3 \text{\texttt{Wind}}+\beta_4 (\text{\texttt{Wind}})^2\\
&+\beta_5 \text{\texttt{Solar.R}}+\beta_6 (\text{\texttt{Solar.R}})^2+\epsilon 
\end{split}
\end{equation*}
where $\epsilon\sim\mathcal{N}(0,\sigma^2)$. The noise variance $\sigma^2$ is assumed to be unknown so that the model contains 8 parameters that need to be estimated from the data.


The left plot in Figure \ref{figs:ozone1} shows the distribution of the observed values for the response variable $\log(\text{\texttt{Ozone}})$. From this plot we observe that there is one isolated observation (i.e.~outlier), for which the observed value of $\log(\text{\texttt{Ozone}})$ is  0.


### Data preparation

We start by loading   the  {\sc \textbf{regMMD}}  package and the dataset:
\begin{verbatim}
R> require("regMMD")
R> air.data <- na.omit(airquality)
\end{verbatim}
and we prepare the vector {\tt y} containing the observations for the response variable as well as the design  matrix {\tt X}:
\begin{verbatim}
R> y <- log(air.data[,1])
R> X <- as.matrix(air.data[,-c(1,5,6)])
R> X <- cbind(poly(air.data[,2], degree=2),poly(air.data[,3], degree=2),
            poly(air.data[,4], degree=2))
\end{verbatim}

### OLS estimation

We first estimate the model with the ordinary least squares (OLS) approach, using both   the full dataset and the one obtained by removing the outlier:
\begin{verbatim}
R> ols.full <- lm(y~X)
R> ii <- which(y<1)
R> ols <- lm(y[-ii]~X[-ii,])
R> print(cbind(ols.full$coefficients,ols$coefficients))
                 [,1]       [,2]
(Intercept)  3.4159273  3.4361465
X1           2.6074060  2.2885974
X2          -1.2034322 -0.8487403
X1          -2.2633108 -2.5209875
X2           1.2894833  1.1009511
X1           4.2120018  3.9218225
X2           0.4501812  0.8936152

\end{verbatim}

As expected, the OLS estimates are sensitive to the presence of outliers in the data. In particular, we observe that the unique outlier in the data has a non-negligible impact on the estimated regression coefficient of the variables $(\text{\texttt{Temp}})^2 $ and $(\text{\texttt{Solar.R}})^2$.

### MMD estimation with the default settings\label{sub:MMD_default}
 
Using the default settings of the \texttt{mmd\_reg} function, the computationally cheap  estimator $\tilde{\theta}_{\mathrm{reg}}$ with a Gaussian kernel $k_Y(y,y')$ is used, and the model is estimated as follows:
\begin{verbatim}
R> mmd.tilde <- mmd_reg(y,X)
R> summary(mmd.tilde)
======================== Summary ========================
Model:               linearGaussian
Estimator:           theta tilde (bdwth.x=0)
---------------------------------------------------------
  Coefficients           Estimate
---------------------------------------------------------
  (Intercept)               3.427
  X1                       2.3448
  X2                      -0.8132
  X3                       -2.329
  X4                       1.0565
  X5                       4.1788
  X6                       0.8369
---------------------------------------------------------
  Std. dev. of Gaussian noise : 0.4484 (estimated)
---------------------------------------------------------
  Kernel for y: Gaussian with bandwidth 0.5821
========================================================= 
\end{verbatim}

As expected from the robustness properties of MMD based estimators, we observe that the estimated values of the regression coefficients are similar to those obtained by OLS on the dataset without the outlier. The \texttt{summary} command also  returns the value of the   bandwidth parameter $\gamma_Y$ obtained with the median heuristic and used to compute the estimator. 

To estimate the model using the alternative estimator $\hat{\theta}_{\mathrm{reg}}$ we need to choose a non-zero value for  \texttt{bdwth.x}. Using the default setting, this estimator is computed as follows:
\begin{verbatim}
R> mmd.hat <- mmd_reg(y,X,bdwth.x="auto")
R> summary(mmd.hat)
======================== Summary ========================
Model:               linearGaussian
Estimator:           theta hat  (bdwth.x>0)
---------------------------------------------------------
  Coefficients           Estimate
---------------------------------------------------------
  (Intercept)              3.4269
  X1                       2.3444
  X2                      -0.8131
  X3                      -2.3297
  X4                        1.056
  X5                       4.1786
  X6                       0.8374
---------------------------------------------------------
  Std. dev. of Gaussian noise : 0.4482 (estimated)
---------------------------------------------------------
  Kernel for y: Gaussian with bandwidth 0.5821
  Kernel for x: Laplace with bandwidth 0.0215
========================================================= 
\end{verbatim}

We remark that the  value for  \texttt{bdwth.x} selected by the default setting is very small, and thus the two MMD estimators provide very similar estimates of the model parameters.

### Tuning the fit in MMD estimation

Above, the estimator $\hat{\theta}_{\mathrm{reg}}$ was computed using  a Laplace kernel  for  $k_X(x,x')$ and  a Gaussian kernel $k_Y(y,y')$, and using a data driven approach to select their bandwidth parameters $\gamma_X$ and $\gamma_Y$.  If, for instance, we want to use  a Cauchy kernel  for $k_X(x,x')$  with bandwidth parameter $\gamma_X=0.2$  and a Laplace kernel for $k_Y(y,y')$  with bandwidth parameter $\gamma_Y=0.5$, we can proceed as follows:
\begin{verbatim}
R> mmd.hat <- mmd_reg(y,X,bdwth.x=0.1,bdwth.y=0.5,kernel.x="Cauchy",
 					                    kernel.y="Laplace")
R> summary(mmd.hat)
======================== Summary ========================
Model:               linearGaussian
Estimator:           theta hat  (bdwth.x>0)
---------------------------------------------------------
  Coefficients           Estimate
---------------------------------------------------------
  (Intercept)              3.4189
  X1                       2.3633
  X2                      -0.7652
  X3                      -2.3423
  X4                       1.0684
  X5                       4.2731
  X6                       0.8787
---------------------------------------------------------
  Std. dev. of Gaussian noise : 0.4492 (estimated)
---------------------------------------------------------
  Kernel for y: Laplace with bandwidth 0.5
  Kernel for x: Cauchy with bandwidth 0.1
========================================================= 
\end{verbatim}

We recall that different choices for the kernels $k_Y(y,y')$ and $k_X(x,x')$ lead to different MMD estimators, which explains why the estimates obtained here are different from those obtained in Section \ref{sub:MMD_default}.


## Robust Poisson regression

### Dataset and model

As a last example we consider the same dataset and task as in Section \ref{sub:Lin_Reg}, but now assume the following Poisson regression model:
\begin{equation*}
\begin{split}
\text{\texttt{Ozone}} \sim\mathrm{Poisson}\bigg(\exp\Big(\alpha&+ \beta_1\text{\texttt{Temp}}+\beta_2(\text{\texttt{Temp}})^2+\beta_3 \text{\texttt{Wind}}+\beta_4 (\text{\texttt{Wind}})^2\\
&+\beta_5 \text{\texttt{Solar.R}}+\beta_6 (\text{\texttt{Solar.R}})^2\Big)\bigg).
\end{split}
\end{equation*}
Noting that   the response variable is now \texttt{Ozone}  and not $\log (\text{\texttt{Ozone}})$ as in  Section \ref{sub:Lin_Reg}, we modify the vector  {\tt y} accordingly:
\begin{verbatim}
R> y <- exp(y)
\end{verbatim}


The right plot in Figure \ref{figs:ozone1} shows the distribution of the observed values for the response variable \texttt{Ozone}. From this plot we observe that there is one isolated observation (i.e.~outlier), for which the observed value of   \texttt{Ozone}  is  larger than 150.


### GLM estimation



We start by estimating the model with the generalized least squares (GLS) approach, using both on the full dataset and  the one obtained by removing the outlier:
\begin{verbatim}
R> glm.full <- glm(y~X,family=poisson)
R> ii <- which(y>150)
R> glm <- glm(y[-ii]~X[-ii,],family=poisson)
R> print(cbind(glm.full$coefficients,glm$coefficients))
                 [,1]         [,2]
(Intercept)  3.5168945  3.506950377
X1           2.4621470  2.332854309
X2          -0.8796456 -0.827578293
X1          -2.4753279 -2.167010600
X2           0.9498002  0.559589428
X1           4.0664346  4.289933463
X2          -0.3247376 -0.003300254
\end{verbatim}

It is well-known that the GLM estimator is sensitive to outliers and, as a result, we observe that the unique outlier present in the data has a non-negligible impact on the estimated regression coefficients of the model.


### MMD estimation with default setting



We first estimate the model parameter using the  estimator $\tilde{\theta}_{\mathrm{reg}}$. Using the default setting of \texttt{mmd\_reg} this is done as follow:
\begin{verbatim}
R> mmd.tilde <- mmd_reg(y,X,model="poisson")
R> summary(mmd.tilde)
======================== Summary ========================
Model:               poisson
Estimator:           theta tilde (bdwth.x=0)
---------------------------------------------------------
  Coefficients           Estimate
---------------------------------------------------------
  (Intercept)              3.3703
  X1                       2.4924
  X2                      -1.0414
  X3                      -2.3234
  X4                       0.4451
  X5                       4.7464
  X6                      -0.1005
---------------------------------------------------------
---------------------------------------------------------
  Kernel for y: Laplace with bandwidth 18.3848
========================================================= 
\end{verbatim}

As for the linear regression example, we observe that the estimated values of the regression coefficients are similar to those obtained by GLM on the dataset without the outlier data point.

Finally, we estimate the model parameters using the estimator $\hat{\theta}_{\mathrm{reg}}$ with its default setting:
\begin{verbatim}
R> mmd.hat <- mmd_reg(y,X,model="poisson",bdwth.x="auto")
Warning message:
In mmd_reg(y,X,model= "poisson",bdwth.x ="auto") :
  Warning: The maximum number of iterations  has been reached.
\end{verbatim}
In this case, we obtain a warning message indicating that the maximum number of iterations  \texttt{maxit} allowed for the optimization algorithm has been reach. The value of \texttt{maxit}, set by default to  50\,000,  can be increased using the \texttt{control} argument of \texttt{mmd\_reg}. For instance, setting \texttt{maxit}$=10^6$ remove the warning message:
\begin{verbatim}
R> mmd.hat <- mmd_reg(y,X,model="poisson",bdwth.x="auto",
                     control=list(maxit=10^6))
R> summary(mmd.hat)
======================== Summary ========================
Model:               poisson
Estimator:           theta hat  (bdwth.x>0)
---------------------------------------------------------
  Coefficients           Estimate
---------------------------------------------------------
  (Intercept)              3.3665
  X1                        2.503
  X2                       -1.075
  X3                       -2.418
  X4                       0.2654
  X5                       4.7292
  X6                      -0.1129
---------------------------------------------------------
---------------------------------------------------------
  Kernel for y: Laplace with bandwidth 18.3848
  Kernel for x: Laplace with bandwidth 0.0215
========================================================= 
\end{verbatim}
As for the linear regression model, the value of  \texttt{bdwth.x} selected by the data driven approach implemented in \texttt{mmd\_reg} is close to zero and the estimate values of the model parameters are therefore similar to those above obtained with the estimator $\tilde{\theta}_{\mathrm{reg}}$.


# References {.unnumbered}

::: {#refs}
:::

# Session information {.appendix .unnumbered}

```{r session-info}
sessionInfo()
```

